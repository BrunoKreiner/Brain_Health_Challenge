{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tcy271XRf-6r",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-On Workbook: Deep Learning in Medicine\n",
    "\n",
    "Lecturer: Susanne Suter (susanne.suter@fhnw.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNnoOy5y9kA8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reference: this notebook was inspired by https://appliedmldays.org/workshops/machine-learning-for-smart-dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### General information to run this notebook\n",
    "\n",
    "- For this notebook, the Tensorflow image is required on the FHNW JupyterHub server (https://jhub.cs.technik.fhnw.ch/).\n",
    "- Locally, the dependencies can be loaded via the Docker Tensorflow image (https://hub.docker.com/r/tensorflow/tensorflow/).\n",
    "- The notebook can alternatively be used on Google Colab (https://research.google.com/colaboratory/).\n",
    "- GPU resources are an advantage (e.g. via Google Colab), but are not required.\n",
    "- Video tutorial for this notebook: https://tube.switch.ch/videos/p456sXZPq6 (Attention: there is a small bug with the sklearn Confusion Matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxMkO9Hzu60Q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tumor Classification using a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPhFAMt_ZJ2M",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this exercise, we will create and tune a convolutional neural network (CNN) that is able to detect the tumors in brain images acquired using MRI. \n",
    "\n",
    "You will be gently guided through every step to create the trained CNN classifier. Parameters that can be tuned are marked with <font color='blue'>TODO</font>. Questions to be answered are marked with <font color='blue'>Question</font>.\n",
    "\n",
    "The ground truth data is not perfect, hence, this will allow you to capture the essential points about the ground truth data generation.\n",
    "\n",
    "The ground truth data contains of the labels (answers) inserted into the CNN that are used for training. That is, the ground truth represents the true y values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Question</font> Is this a regression or classification task that we model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage import io\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import os, glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sG-eGQ89x0S0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importing Data\n",
    "\n",
    "This time, we will import the data for this exercise directly from a git repository. \n",
    "\n",
    "The MRI images (with and without tumor) are for teaching purposes prepared from the below source data. They are already sorted in two directories for our two classes to classify/predict.\n",
    "\n",
    "The ownership of the data is: \"Brain MRI Images for Brain Tumor Detection\"\n",
    "https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection\n",
    "Navoneel Chakrabarty (2019), version 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mkguOK2CYN4b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "fatal: Too many arguments.\n",
      "\n",
      "usage: git clone [<options>] [--] <repo> [<dir>]\n",
      "\n",
      "    -v, --verbose         be more verbose\n",
      "    -q, --quiet           be more quiet\n",
      "    --progress            force progress reporting\n",
      "    -n, --no-checkout     don't create a checkout\n",
      "    --bare                create a bare repository\n",
      "    --mirror              create a mirror repository (implies bare)\n",
      "    -l, --local           to clone from a local repository\n",
      "    --no-hardlinks        don't use local hardlinks, always copy\n",
      "    -s, --shared          setup as shared repository\n",
      "    --recurse-submodules[=<pathspec>]\n",
      "                          initialize submodules in the clone\n",
      "    --recursive ...       alias of --recurse-submodules\n",
      "    -j, --jobs <n>        number of submodules cloned in parallel\n",
      "    --template <template-directory>\n",
      "                          directory from which templates will be used\n",
      "    --reference <repo>    reference repository\n",
      "    --reference-if-able <repo>\n",
      "                          reference repository\n",
      "    --dissociate          use --reference only while cloning\n",
      "    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n",
      "    -b, --branch <branch>\n",
      "                          checkout <branch> instead of the remote's HEAD\n",
      "    -u, --upload-pack <path>\n",
      "                          path to git-upload-pack on the remote\n",
      "    --depth <depth>       create a shallow clone of that depth\n",
      "    --shallow-since <time>\n",
      "                          create a shallow clone since a specific time\n",
      "    --shallow-exclude <revision>\n",
      "                          deepen history of shallow clone, excluding rev\n",
      "    --single-branch       clone only one branch, HEAD or --branch\n",
      "    --no-tags             don't clone any tags, and make later fetches not to follow them\n",
      "    --shallow-submodules  any cloned submodules will be shallow\n",
      "    --separate-git-dir <gitdir>\n",
      "                          separate git dir from working tree\n",
      "    -c, --config <key=value>\n",
      "                          set config inside the new repository\n",
      "    --server-option <server-specific>\n",
      "                          option to transmit\n",
      "    -4, --ipv4            use IPv4 addresses only\n",
      "    -6, --ipv6            use IPv6 addresses only\n",
      "    --filter <args>       object filtering\n",
      "    --remote-submodules   any cloned submodules will use their remote-tracking branch\n",
      "    --sparse              initialize sparse-checkout file to include only files at root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import MRI data (includes independent benchmark data)\n",
    "# Note: only import this data once (not every time)\n",
    "\n",
    "!rm -fr MRI_data* # removes the directory MRI_data in case it already exists\n",
    "\n",
    "!git clone https://github.com/susuter/MRI_data.git # clones the git repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMhAAfob9Kcf",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The loaded data contains three folders with MRI brain images in png format. You can explore them by browsing the directories next to this notebook file.\n",
    "\n",
    "* Folder `yes` contains images with tumors\n",
    "* Folder `no` contains images without tumors\n",
    "* Folder `benchmark` contains independent test images\n",
    "\n",
    "Note: Ideally, the input images are all of the same pixel width and height. This way, we profit from the best resolution available from this data.\n",
    "\n",
    "<font color='blue'>Question</font>: What are potential problems, when the input data is not of the same image size?\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y5C2mO7Up5R7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'Z:\\brain-health\\fhnw_ds_fs22_cdl1_cml1_Brain_Health_Challenge\\notebooks\\0_eda\\MRI_data\\pos\\Pos1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10640/963797741.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# TODO: display other images yourself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mimg_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MRI_data/pos/Pos1.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mimg_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MRI_data/neg/Neg2.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\io\\_io.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imread'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ndim'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\io\\manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m                                (plugin, kind))\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;31m# Get reader and read first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[1;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# Create request object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;31m# Get format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\request.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Parse what was given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m# Set extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    258\u001b[0m                 \u001b[1;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file: '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                 \u001b[1;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file: 'Z:\\brain-health\\fhnw_ds_fs22_cdl1_cml1_Brain_Health_Challenge\\notebooks\\0_eda\\MRI_data\\pos\\Pos1.png'"
     ]
    }
   ],
   "source": [
    "# Show an image of each dataset to make sure that the data is correctly loaded \n",
    "# This gives an idea on how the images look\n",
    "\n",
    "# TODO: display other images yourself\n",
    "img_1 = io.imread(\"MRI_data/pos/Pos1.png\")\n",
    "img_2 = io.imread(\"MRI_data/neg/Neg2.png\")\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.imshow(img_1)\n",
    "ax1.set_title(\"Example image with tumor\")\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.imshow(img_2)\n",
    "ax2.set_title(\"Example image without tumor\")\n",
    "\n",
    "\n",
    "# Display the size of the images. In python this is called \"shape\"\n",
    "# (pixel rows, pixel columns)\n",
    "\n",
    "print( \"Shape of image with tumor: \")\n",
    "print( img_1.shape )\n",
    "\n",
    "print( \"Shape of image without tumor: \")\n",
    "print( img_2.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font color='blue'>Question</font>: Do you see the tumor in the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wrAgUPMBHyp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to count number of png images in given directory\n",
    "\n",
    "def count_pngs_in_directory(dir_name):\n",
    "  print( \"Number of png images in directory '\", dir_name, \"': \")\n",
    "  !cd $dir_name; ls -l *.png | wc -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6S7oR-rJSjno",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print the number of source images available for each dataset\n",
    "\n",
    "count_pngs_in_directory(\"MRI_data/pos\")\n",
    "count_pngs_in_directory(\"MRI_data/neg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN67Z4fYx9CY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Augmenting the Available Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSZlFZ6LXfBZ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data augmentation is performed in order to make your ground truth dataset more diverse such that the computed algorithm is more robust to new data - not used for the training. Typical data augmentation modifications include adjusting brightness, color, contrast, distorions, adding noise, zooming, rotations, or mirroring.\n",
    "\n",
    "There are various ready made data augmentor tools available. In case you need to augment also your labels, make sure you choose an augmentor tool that is able to do so. The one presented here can only augment raw images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy69ainzg0ip",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import Augmentor (library to create more distorted images)\n",
    "!pip install Augmentor\n",
    "import Augmentor\n",
    "\n",
    "# Define a function to create augmented images images\n",
    "# TODO: change parameters of augmentor\n",
    "def build_augmented_images(path_folder, n_samples):\n",
    "    p = Augmentor.Pipeline(path_folder)\n",
    "    p.rotate(probability=0.3, max_left_rotation=4, max_right_rotation=4)\n",
    "    p.zoom(probability=0.3, min_factor=0.7, max_factor=1.2) \n",
    "    p.random_distortion(probability=0.3, grid_width=4, grid_height=4, magnitude=6)\n",
    "    p.random_brightness(probability=0.8, min_factor=0.5, max_factor=2)\n",
    "    p.random_color(probability=0.3, min_factor=0.5, max_factor=1.5)\n",
    "    p.random_contrast(probability=0.3, min_factor=0.8, max_factor=1.2)\n",
    "\n",
    "    p.sample(n_samples, multi_threaded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kcz75B0QiZ5E",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Remove previous augmented images, if any (in case you run it twice) \n",
    "!rm -fr MRI_data/pos/output \n",
    "!rm -fr MRI_data/neg/output\n",
    "\n",
    "# Number of augmented images\n",
    "\n",
    "n_y = 500 # number of augmented tumor images\n",
    "n_n = 500 # number of augmented non-tumor images\n",
    "\n",
    "# TODO: choose your own number of augmented images for the two classes\n",
    "\n",
    "# Hint 1: if you start with smaller numbers, the CNN will be trained faster\n",
    "# However, the more images you add, the better accuracy/performace you will achieve\n",
    "# The performance of the CNN can be evaluated in the section \"Prediction Performance\"\n",
    "# Hint 2: consider whether you want to select the same or an unequal number of \n",
    "# images for the two classes\n",
    "\n",
    "# Create augmented images\n",
    "build_augmented_images(\"MRI_data/pos\", n_y)\n",
    "build_augmented_images(\"MRI_data/neg\", n_n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IySng64dUz0m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check number of augmented images of each category\n",
    "\n",
    "count_pngs_in_directory(\"MRI_data/pos/output\")\n",
    "count_pngs_in_directory(\"MRI_data/neg/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLPb8Wp2VFAu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Optionally: list selected augmented images (for checking)\n",
    "\n",
    "!ls -l MRI_data/pos/output/*Pos10*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYyG1FMxUz-k",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Function to display example augmented images\n",
    "\n",
    "def show_augmented_images_example(image_type, image_number):\n",
    "  \n",
    "  base_path = os.path.join('MRI_data', image_type.lower())\n",
    "  augmentation_path = os.path.join(base_path, 'output')\n",
    "\n",
    "  original_filename = os.path.join(base_path, image_type + image_number + '.png')\n",
    "  if not(os.path.exists(original_filename)):\n",
    "    print(\"select a valid image number or type\")\n",
    "    return\n",
    "\n",
    "  original = io.imread(original_filename)\n",
    "  pattern = '*' + image_type + image_number + '*.png';\n",
    "  augmented = fnmatch.filter(os.listdir(augmentation_path), pattern)\n",
    "\n",
    "  fig = plt.figure(figsize=(20,10))\n",
    "  ax1 = fig.add_subplot(1,4,1)\n",
    "  ax1.set_title(\"Original\")\n",
    "  ax1.imshow(original)\n",
    "\n",
    "  if len(augmented) > 0:\n",
    "    augmented1 = io.imread(os.path.join(augmentation_path, augmented[0]))\n",
    "    ax2 = fig.add_subplot(1,4,2)\n",
    "    ax2.imshow(augmented1)\n",
    "    ax2.set_title(\"Augmented\")\n",
    "\n",
    "  if len(augmented) > 1:\n",
    "    augmented2 = io.imread(os.path.join(augmentation_path, augmented[1]))\n",
    "    ax3 = fig.add_subplot(1,4,3)\n",
    "    ax3.imshow(augmented2)\n",
    "    ax3.set_title(\"Augmented\")\n",
    "  \n",
    "  if len(augmented) > 2:\n",
    "    augmented3 = io.imread(os.path.join(augmentation_path, augmented[2]))\n",
    "    ax4 = fig.add_subplot(1,4,4)\n",
    "    ax4.imshow(augmented3)\n",
    "    ax4.set_title(\"Augmented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xgt3y93H46z9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: display exemplar augmented images: change image number and image type\n",
    "image_number = '85'\n",
    "image_type = 'Neg' # 'Pos' or 'Neg'\n",
    "show_augmented_images_example(image_type, image_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOl4hgeFdQGZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qemTxODd8Tq",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this part, we want to make sure that the data is correctly formatted.\n",
    "For example, we need each image to be of the same size with respect to the images dimensions (WIDTH and HEIGHT) and the number of color channels (DEPTH). At the same time, we add the prepared image to a list with our desired file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alw8K_N_TtRY",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the image lists for both classes, tumor and non-tumor\n",
    "image_list_y = glob.glob('MRI_data/pos/output/*.png')\n",
    "image_list_n = glob.glob('MRI_data/neg/output/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smhJVgO_tjJk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function that pre-processes the images for the training\n",
    "\n",
    "# This function does the resize and adds the image to the training set\n",
    "def img_preprocessing(image_list, label, X_, y_):\n",
    "\n",
    "    i = 0\n",
    "    for image in image_list:\n",
    "    \n",
    "        if i%100 == 0: print(\"pre-processing image \",i,\" ...\")\n",
    "    \n",
    "        img = io.imread(image)\n",
    "\n",
    "        img = np.array(img)\n",
    "\n",
    "        # TODO: Optionally change the pixel WIDTH and HEIGHT\n",
    "\n",
    "        # Resize the image; make sure all images have same size\n",
    "        pr = 250 # pixel rows (HEIGHT)\n",
    "        pc = 250 # pixel columns (WIDTH)\n",
    "        img = resize(img, (pr, pc), anti_aliasing=True)\n",
    "\n",
    "        if i == 0:\n",
    "            WIDTH, HEIGHT = np.array(img).shape\n",
    "            print(\"image size:\",WIDTH,HEIGHT)\n",
    "\n",
    "        # Show the first 5 images\n",
    "        if i < 5:\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "    \n",
    "        # Append the pre-processed images to the training set\n",
    "        X_.append(img)\n",
    "        y_.append(label)\n",
    "    \n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8h3GodGuzpB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the training set\n",
    "X_v0 = [] # images\n",
    "y_v0 = [] # labels (ground truth)\n",
    "\n",
    "# Pre-process images and add them to training set\n",
    "print(\"pre-processing tumor images and add them to training set...\")\n",
    "img_preprocessing(image_list_y, [1,0], X_v0, y_v0)\n",
    "print(\"pre-processing non-tumor images and add them to training set...\")\n",
    "img_preprocessing(image_list_n, [0,1], X_v0, y_v0)\n",
    "\n",
    "print(\"Training set is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Shig-TdvCUa",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Change format of the training set from list to numpy array\n",
    "X = np.array(X_v0) # images\n",
    "y = np.array(y_v0) # labels\n",
    "\n",
    "# reshape for tensorflow compatibilty\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font color='blue'>Question</font>: How do you interpret the output of calling `shape` on X or y? How does the output differ from the above output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAbCpvAjwsrJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Defines a function that shows an image from the image_set\n",
    "def show_img(image_set, i):\n",
    "    img = image_set[i, :, :]\n",
    "    img = img.reshape(img.shape[0], img.shape[1])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5jCCANvwzaW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show example images from the training set and check their labels\n",
    "# Label -> what we are predicting\n",
    "# Label [1 0] -> tumor\n",
    "# Label [0 1] -> non-tumor\n",
    "\n",
    "def show_label(i):\n",
    "  show_img(X,i)\n",
    "  print(\"Label: \",  y[i,:])\n",
    "\n",
    "# TODO: select other image indices below\n",
    "\n",
    "# Hint 1: the index range depends on your chosen number of selected augmented images\n",
    "# Hint 2: the tumor images have the lower indices since they were added to the list first\n",
    "\n",
    "# tumor images\n",
    "show_label(10) \n",
    "show_label(22) \n",
    "\n",
    "# non-tumor images\n",
    "show_label(510) \n",
    "show_label(563) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vRt1ogFxMJt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Print X (images) and y (labels) shapes\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDcjewvuziFu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print X (images) and y (labels) values\n",
    "# print(\"Images X: \",X[:5])\n",
    "print(\"Labels y: \",y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INtmefX2gNYF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Split Data into Training Set and Validation Set\n",
    "\n",
    "During the training, the CNN needs certain cross-validation data to compute the loss. What in return makes it possible to update the parameters (weights) of the trained CNN in order to make it perform better. \n",
    "\n",
    "Therefore, we split our ground truth data into a training set and a validation set. Typically, we choose about 20% of the ground truth as validation set. \n",
    "\n",
    "The terminology of the validation set can vary. E.g., it is also called test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxU3vh-DynUT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data from the preprocessed set into a train and test set\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_validation shape: ', X_validation.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_validation shape: ', y_validation.shape)\n",
    "\n",
    "print('Train and test datasets are ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KAb50hkhf_P",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN Model Definition\n",
    "\n",
    "In this section, we define the architecture of our CNN model. That means, we define how many and what type of layers the CNN has. We furthermore define other parameters such as the activation function or our regularization (dropout). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22KbB5JTxMkd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def define_model(num_classes,epochs):\n",
    "    # Create the CNN model\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: Play with the parameters of the layers\n",
    "    \n",
    "    # Hint: Parameters to play with are: \n",
    "    # convolutions, activation functions, dropout, max pooling, ...\n",
    "\n",
    "    # Layer 1 (convolutional plus max pooling)\n",
    "    model.add(Conv2D(4, (5, 5), input_shape=(X.shape[1], X.shape[2], 1), padding='same', activation='relu', kernel_constraint=MaxNorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 2 (convolutional plus max pooling)\n",
    "    model.add(Conv2D(4, (3, 3), activation='relu', padding='same', kernel_constraint=MaxNorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # TODO: Optionally add additional convolutional layers\n",
    "    # ...\n",
    "\n",
    "    # Additional dense layers (fully connected)    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "    # TODO: Optionally choose other (custome-defined) optimizers\n",
    "    \n",
    "    #lrate = 0.005\n",
    "    #decay = lrate/epochs\n",
    "    #sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    #adam = Adam(lr=lrate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay, amsgrad=False)\n",
    "    adam = Adam()\n",
    "    \n",
    "    # Prepares the model and defines the loss and optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAevTL_Z8tka",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the duration of the training process, which is given in epochs\n",
    "# In each epoch the model learns once the whole dataset\n",
    "\n",
    "# TODO: play with the number of epochs that the model is trained for\n",
    "epochs = 10\n",
    "\n",
    "#Create the model\n",
    "model=define_model(2,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzAZ6Wud3Pih",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X25tTYvDzp-n",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history=model.fit(X_train, y_train, validation_data=(X_validation, y_validation), epochs=epochs, batch_size=32)\n",
    "\n",
    "# Plot accuracy vs epochs\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# Plot cost function vs epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model cost function')\n",
    "plt.ylabel('cost function')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_validation, y_validation, verbose=0)\n",
    "print(\"______________________________\")\n",
    "print(\"Validation set accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"______________________________\")\n",
    "# Save the model to a json file\n",
    "model_json = model.to_json()\n",
    "with open(\"model_MRI_v1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model_MRI_v1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80VVstlSadAs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKSWzsrOzxvF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Predictions on the validation sample\n",
    "p_validation = (model.predict(X_validation)>0.5).astype('int32')\n",
    "\n",
    "# Predictions on the training sample\n",
    "p_train = (model.predict(X_train)>0.5).astype('int32')\n",
    "\n",
    "print('Confusion matrix for the train set:')\n",
    "print(confusion_matrix(y_train[:,0], p_train[:,0]))\n",
    "\n",
    "print('Confusion matrix for the validation set:')\n",
    "print(confusion_matrix(y_validation[:,0], p_validation[:,0]))\n",
    "\n",
    "print('[[true positives, false negatives]')\n",
    "print('[false positives, true negatives]]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hONTJTGKGkT8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def show_example_prediction(X_, y_,p_,i):\n",
    "  show_img(X_,i)\n",
    "  print(\"Label:       \",y_[i,:])\n",
    "  print(\"Prediction:  \",p_[i,:])\n",
    "\n",
    "# TODO: choose other image indices below (last parameter)\n",
    "# TODO: find false positives and false negatives\n",
    "show_example_prediction(X_validation,y_validation,p_validation,100)\n",
    "show_example_prediction(X_validation,y_validation,p_validation,102)\n",
    "show_example_prediction(X_validation,y_validation,p_validation,104)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxYXkxDWjoZE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independent Benkmark Test Set\n",
    "\n",
    "Now, the trained CNN model is evaluated on an independent test dataset, which was not used for to calculated the loss during the training.\n",
    "\n",
    "To differentiate from the terminology test set, which was used during the training, we use here the terminology benchmark test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YqR3gcJa3S8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the benchmark test set\n",
    "\n",
    "import glob\n",
    "\n",
    "# Generate the image lists for both benchmark classes, female and male\n",
    "# The source data was downloaded previously from the repository\n",
    "image_list_bench_f = glob.glob('MRI_data/benchmark/pos/*.png')\n",
    "image_list_bench_m = glob.glob('MRI_data/benchmark/neg/*.png')\n",
    "\n",
    "# Initialize the benchmark set\n",
    "X_bench_v0 = [] # images\n",
    "y_bench_v0 = [] # labels (ground truth)\n",
    "\n",
    "# Pre-process images and add them to benchmark set\n",
    "print(\"pre-processing tumor images and add them to benchmark set...\")\n",
    "img_preprocessing(image_list_bench_f, [1,0], X_bench_v0, y_bench_v0)\n",
    "print(\"pre-processing non-tumor images and add them to benchmark set...\")\n",
    "img_preprocessing(image_list_bench_m, [0,1], X_bench_v0, y_bench_v0)\n",
    "\n",
    "# Change format of the benchmark set from list to numpy array\n",
    "X_bench = np.array(X_bench_v0) # images\n",
    "y_bench = np.array(y_bench_v0) # labels\n",
    "\n",
    "print(\"Benchmark set is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMqAUeuRtigk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the images of the benchmark test set\n",
    "X_bench = X_bench.reshape(X_bench.shape[0], X_bench.shape[1],X_bench.shape[2], 1)\n",
    "p_bench = (model.predict(X_bench)) # probabilities\n",
    "p_bench_c = (p_bench>0.5).astype('int32') # classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okuiF20Svkr7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance of Benchmark Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksht9i1anjlO",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define functions to calculate performance\n",
    "\n",
    "# note: handling division by zero for readability not implemented\n",
    "\n",
    "# accuracy\n",
    "def calc_accuracy(tp,tn,n):\n",
    "  return (tp+tn)/n\n",
    "\n",
    "# harmonic mean of precision and recall\n",
    "def calc_f1_score(tp,fp,fn):\n",
    "  return (2*tp)/(2*tp + fp + fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXRSgtlDnZHm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate performance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('Confusion matrix for the benchmark test set')\n",
    "cf = confusion_matrix(y_bench[:,0], p_bench_c[:,0])\n",
    "print(cf)\n",
    "print('[[true negatives, false negatives]')\n",
    "print('[false positives, true positives]]')\n",
    "\n",
    "n = y_bench.shape[0]\n",
    "tp = cf[1,1]\n",
    "tn = cf[0,0]\n",
    "fp = cf[1,0]\n",
    "fn = cf[0,1]\n",
    "\n",
    "print(\"Total samples        : \",n)\n",
    "print(\"True positives (tp)  : \",tp) # it is really a tumor\n",
    "print(\"False negatives (fn) : \",fn) # it is classified as non-tumor, but is a tumor\n",
    "print(\"False positives (fp) : \",fp) # it is classified as tumor, but is a non-tumor\n",
    "print(\"True negatives (tn)  : \",tn) # it really is not a tumor, i.e., it is a non-tumor\n",
    "\n",
    "print(\"Accuracy             : \",calc_accuracy(tp,tn,n))\n",
    "print(\"F1 score             : \",calc_f1_score(tp,fp,fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm1untSld6Ar",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def show_example_prediction(X_, y_,p_,i):\n",
    "  show_img(X_,i)\n",
    "  print(\"Label:       \",y_[i,:])\n",
    "  print(\"Prediction:  \",p_[i,:])\n",
    "\n",
    "# tumor (smaller indices - depends on your total ground truth size)\n",
    "show_example_prediction(X_bench,y_bench,p_bench_c,0)\n",
    "show_example_prediction(X_bench,y_bench,p_bench_c,1)\n",
    "show_example_prediction(X_bench,y_bench,p_bench_c,2)\n",
    "\n",
    "# non-tumor (larger indices - depends on your total ground truth size)\n",
    "show_example_prediction(X_bench,y_bench,p_bench_c,5) \n",
    "show_example_prediction(X_bench,y_bench,p_bench_c,6) \n",
    "show_example_prediction(X_bench,y_bench,p_bench_c,7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OS2I4racC3Bk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenge: Optimize the CNN Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kf8_NGWdB5BR",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Challenge:\n",
    "optimize the model to have the highest accuracy on the test set (test set accuracy)\n",
    "\n",
    "* Parameters to vary:\n",
    "  * Number of augmented images / number of images in training set\n",
    "  * Ratio between tumor and non-tumor images\n",
    "  * Selection of images for training\n",
    "  * (Resizing of images)\n",
    "  * Epochs\n",
    "  * For the convolutional layers:\n",
    "    * Number of convolutional layers\n",
    "    * Number of filters in a layer\n",
    "    * Size of layers (3x3, 5x5, ...)\n",
    "    * Activation functions\n",
    "  * Adjust the optimizer and add manual parameters (gradient descent)\n",
    "\n",
    "* Rule of thumb: number of parameters of the model should be lower than the training set size.\n",
    "\n",
    "* Hint: Good configurations of the CNN model are typically found by searching the internet for existing working architectures."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "aiml_Deep-Learning-in-Medicine-Workbook_2021-04-15.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
