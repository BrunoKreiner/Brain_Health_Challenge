{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train Test Split\n",
    "This notebook contains functionality to split the images into a train and test set and to create an annotation csv-file used by the pytorch data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, '../../scripts/')\n",
    "\n",
    "from helpers import miscellaneous as misc\n",
    "\n",
    "CONFIG = misc.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split with label CN, MCI and AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Data ID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Visit</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>Acq Date</th>\n",
       "      <th>Format</th>\n",
       "      <th>Downloaded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I97327</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>3/02/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I112538</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>6/01/2008</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I97341</td>\n",
       "      <td>941_S_1311</td>\n",
       "      <td>MCI</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>9/27/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I63874</td>\n",
       "      <td>941_S_1202</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR-R; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>1/30/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I75150</td>\n",
       "      <td>941_S_1202</td>\n",
       "      <td>CN</td>\n",
       "      <td>M</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>MRI</td>\n",
       "      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n",
       "      <td>Processed</td>\n",
       "      <td>8/24/2007</td>\n",
       "      <td>NiFTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Image Data ID     Subject Group Sex  Age  Visit Modality  \\\n",
       "0        I97327  941_S_1311   MCI   M   69      1      MRI   \n",
       "1       I112538  941_S_1311   MCI   M   70      4      MRI   \n",
       "2        I97341  941_S_1311   MCI   M   70      3      MRI   \n",
       "3        I63874  941_S_1202    CN   M   78      1      MRI   \n",
       "4        I75150  941_S_1202    CN   M   78      3      MRI   \n",
       "\n",
       "                                  Description       Type   Acq Date Format  \\\n",
       "0    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  3/02/2007  NiFTI   \n",
       "1    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  6/01/2008  NiFTI   \n",
       "2  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  9/27/2007  NiFTI   \n",
       "3  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  1/30/2007  NiFTI   \n",
       "4    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  8/24/2007  NiFTI   \n",
       "\n",
       "   Downloaded  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image descriptions\n",
    "df = pd.read_csv('../../' + CONFIG['RAW_DATA_DIR'] + '/images/ADNI1_Complete_1Yr_1.5T_1_20_2022.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I40845']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_file_name = 'ADNI_002_S_1155_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070217034919863_S24144_I40845.nii'\n",
    "\n",
    "regex_pattern = '(I[0-9]*)\\.nii'  # extracts image data id from file name\n",
    "\n",
    "re.findall(regex_pattern, example_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(88, 94), match='I40845'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('I40845', example_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nii_files = misc.get_nii_filenames(CONFIG['FLATTENED_DATA_DIR'])\n",
    "nii_files = [i.split(\"../\")[-1] for i in nii_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/raw/flattened\\\\ADNI_002_S_0295_MR_MPR__GradWarp__B1_Correction__N3__Scaled_2_Br_20081001114556321_S13408_I118671.nii'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nii_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_into_train_test(df, group_identifier, data_dir, cognitive_test_path = None, proportion=0.8, save_files=False, file_type = \"nii\"):\n",
    "    \"\"\"\n",
    "    df: file to ADNI images dataset\n",
    "    group_identifier: identifier for CN, MCI or AD\n",
    "    data_dir: path to all .nii files of subjects\n",
    "    cognitive_test_path: if not none: add to each image a cognitive test result based on the given test, default: None\n",
    "    proportion: train/test split proportion, default: 0.8\n",
    "    save_files: save as file (True/False), default: False\n",
    "    \"\"\"\n",
    "    assert 1 > proportion > 0, f'parameter proportion has to be in range [0,1]. {proportion} was given'\n",
    "\n",
    "    testset = pd.DataFrame(columns=['filename', 'Group', 'Subject'])\n",
    "    trainset = pd.DataFrame(columns=['filename', 'Group', 'Subject'])\n",
    "\n",
    "    # add filenames to df\n",
    "    if file_type == \"png\":\n",
    "        filenames = misc.get_png_filenames(data_dir)\n",
    "    else:\n",
    "        filenames = misc.get_nii_filenames(data_dir)\n",
    "    \n",
    "    filenames = [i.split(\"../\")[-1] for i in filenames]\n",
    "\n",
    "    def get_filename_by_id(x):\n",
    "        filename = [file for file in filenames if x['Image Data ID'] in file]\n",
    "        if len(filename) == 1:\n",
    "            return filename[0]\n",
    "    #   else:\n",
    "    #       print(x['Image Data ID']) # fixme missing images?\n",
    "\n",
    "    df['filename'] = df.apply(get_filename_by_id, axis=1)\n",
    "\n",
    "    # group by identifier\n",
    "    df_grouped = df.groupby(group_identifier)\n",
    "\n",
    "    groups = df[group_identifier].unique()\n",
    "    for group in groups:\n",
    "        df_group = df_grouped.get_group(group)\n",
    "        subjects = df_group['Subject'].unique()\n",
    "        random.seed(CONFIG['RANDOM_STATE'])\n",
    "        random.shuffle(subjects)\n",
    "\n",
    "        n_train = int(len(subjects) * proportion)  # accordingly n_test = len(subjects) - n_train\n",
    "\n",
    "        train_subjects = subjects[:n_train]\n",
    "        test_subjects = subjects[n_train:]\n",
    "\n",
    "        sub_trainset = df_group[df_group['Subject'].isin(train_subjects)]\n",
    "        sub_testset = df_group[df_group['Subject'].isin(test_subjects)]\n",
    "        \n",
    "        trainset_cognitive_test_column_list = []\n",
    "        testset_cognitive_test_column_list = []\n",
    "        \n",
    "        if(cognitive_test_path is not None):\n",
    "            sub_trainset, trainset_cognitive_test_column_list = get_cognitive_test_set(sub_trainset, cognitive_test_path)\n",
    "            sub_testset, testset_cognitive_test_column_list = get_cognitive_test_set(sub_testset, cognitive_test_path)\n",
    "        \n",
    "        trainset = pd.concat([trainset, sub_trainset[['filename', 'Group', 'Subject'] + trainset_cognitive_test_column_list]], ignore_index=True)\n",
    "        testset = pd.concat([testset, sub_testset[['filename', 'Group', 'Subject'] + testset_cognitive_test_column_list]], ignore_index=True)\n",
    "        \n",
    "        \n",
    "    if save_files:\n",
    "        trainset.to_csv('../../' + CONFIG['ANNOTATIONS_DIR'] + 'train_labels.csv', sep=',', header=True, index=False)\n",
    "        testset.to_csv('../../' + CONFIG['ANNOTATIONS_DIR'] + 'test_labels.csv', sep=',', header=True, index=False)\n",
    "\n",
    "        return None\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "def get_cognitive_test_set(subjects, path_to_cognitive_test_set):\n",
    "    \"\"\"\n",
    "    Merges the train or test set entries with their cognitive test result data by looking at the date of the USERDATE attribute.\n",
    "    Mathces image date of acquisition to the closest cognitive test result in time.\n",
    "    TODO: see if USERDATE is adequate to match with \"acq date\" attribute from the images.\n",
    "    \"\"\"\n",
    "    subjects[\"Subject_ID\"] = subjects[\"Subject\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    subjects[\"Acq Date\"] = pd.to_datetime(subjects[\"Acq Date\"])\n",
    "\n",
    "    cognitive_test_results = pd.read_csv(path_to_cognitive_test_set)\n",
    "\n",
    "    cognitive_test_results.USERDATE = pd.to_datetime(cognitive_test_results.USERDATE)\n",
    "\n",
    "    #fill dictionary with all the test results by matching subject_id with RID\n",
    "    dicts = {}\n",
    "    for item in subjects[\"Subject_ID\"].unique(): dicts[item]=cognitive_test_results[cognitive_test_results.RID == int(item)]\n",
    "\n",
    "    #this could potentially replace all of the active code below\n",
    "    \"\"\"subjects[cognitive_test_results.columns] = np.nan\n",
    "    for i in range(0, len(subjects)):\n",
    "        value = get_closest_test_result_by_date(subjects.iloc[i], dicts)\n",
    "        subjects.loc[i, cognitive_test_results.columns.values.tolist()] = value\"\"\"\n",
    "    \n",
    "    closest_test_results = [get_closest_test_result_by_date(subjects.iloc[i], dicts) for i in range(0, len(subjects))]\n",
    "    closest_test_results = pd.concat(closest_test_results).reset_index(drop=True)\n",
    "    subjects = subjects.reset_index(drop=True)\n",
    "    subjects_ = pd.concat([subjects, closest_test_results], axis=1)\n",
    "    return subjects_, cognitive_test_results.columns.values.tolist()\n",
    "\n",
    "def get_closest_test_result_by_date(item, dicts):\n",
    "    #Needs check when it doesn't find anything in the list of all cognitive test results\n",
    "    try:\n",
    "        test_results = dicts[item.Subject_ID]\n",
    "        test_results = test_results[test_results.USERDATE ==  min(test_results.USERDATE, key = lambda x: abs(x - item[\"Acq Date\"]))]\n",
    "        return(test_results)\n",
    "    except:\n",
    "        print(f\"test_result not found for image subject ID {item.Subject_ID}, returning NONE\")\n",
    "        #Implement if needed\n",
    "        value = next(iter(dicts.values()))\n",
    "        value = pd.DataFrame(columns = value.columns)\n",
    "        value = value.append([np.nan], ignore_index=True)\n",
    "        return(value)\n",
    "\n",
    "split_into_train_test(df, 'Group', CONFIG['FLATTENED_DATA_DIR'], save_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split according to granular diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# maybe later ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
