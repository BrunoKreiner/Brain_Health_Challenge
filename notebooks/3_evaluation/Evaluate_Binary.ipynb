{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image\n",
    "import torchio as tio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sys.path.insert(0, '../../scripts/')\n",
    "\n",
    "from helpers import miscellaneous as m\n",
    "from helpers import preprocessing2d as prep\n",
    "from helpers import plotters\n",
    "from data_loader import MRIDatasetSlice\n",
    "from ml_models import get_model\n",
    "from loss_functions import get_optimizer, get_criterion\n",
    "import numpy as np\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "\n",
    "\n",
    "CONFIG = m.get_config()\n",
    "MODEL_PATH = '../../models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Group</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_0851_MR_MPR__Gra...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_0851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_0851_MR_MPR-R__G...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_0851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename Group     Subject\n",
       "0  data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...   MCI  141_S_1052\n",
       "1  data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...   MCI  141_S_1052\n",
       "2  data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...   MCI  141_S_1052\n",
       "3  data/raw/flattened\\ADNI_141_S_0851_MR_MPR__Gra...   MCI  141_S_0851\n",
       "4  data/raw/flattened\\ADNI_141_S_0851_MR_MPR-R__G...   MCI  141_S_0851"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv('../../data/annotations/test_labels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'ResNet18_CV_LR0.0014926327291968032_CRFocal Loss_TRMonai_Blur_OPAdamW_BS16_EP30__ID_d066a171-10e9-4d9b-8999-c93a0a982283.pth'\n",
    "model = get_model(\"ResNet18\")\n",
    "\n",
    "TRANSFORMER_NAME = \"Monai_Blur\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "CRITERION = \"Focal Loss\"\n",
    "OPTIMIZER = \"AdamW\"\n",
    "\n",
    "criterion = get_criterion({\"CRITERION\":CRITERION})\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "transformer = prep.get_transformer(TRANSFORMER_NAME)[0]\n",
    "\n",
    "state_dict = torch.load(MODEL_PATH + MODEL_NAME)\n",
    "\n",
    "#state_dict\n",
    "model.load_state_dict(state_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = MRIDatasetSlice('../../data/annotations/test_labels.csv','../../' + CONFIG['FLATTENED_DATA_DIR'], transformer,  'Center', 3)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Group</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_0851_MR_MPR__Gra...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_0851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/raw/flattened\\ADNI_141_S_0851_MR_MPR-R__G...</td>\n",
       "      <td>MCI</td>\n",
       "      <td>141_S_0851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename Group     Subject\n",
       "0  data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...   MCI  141_S_1052\n",
       "1  data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...   MCI  141_S_1052\n",
       "2  data/raw/flattened\\ADNI_141_S_1052_MR_MPR__Gra...   MCI  141_S_1052\n",
       "3  data/raw/flattened\\ADNI_141_S_0851_MR_MPR__Gra...   MCI  141_S_0851\n",
       "4  data/raw/flattened\\ADNI_141_S_0851_MR_MPR-R__G...   MCI  141_S_0851"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testset = pd.read_csv('../../data/annotations/test_labels.csv')\n",
    "df_testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(result); plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m); plt\u001b[38;5;241m.\u001b[39mtight_layout(); plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     34\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSLICED_DATA_DIR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter/3/\u001b[39m\u001b[38;5;124m'\u001b[39m , df_testset\u001b[38;5;241m.\u001b[39mfilename[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw/flattened\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 35\u001b[0m \u001b[43mGradCam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mGradCam\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m image \u001b[38;5;241m=\u001b[39m tio\u001b[38;5;241m.\u001b[39mScalarImage(img_path)\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mgrad_cam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonai_Blur\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mgrad_cam\u001b[1;34m(model, target_layer, image, transformer_name)\u001b[0m\n\u001b[0;32m     24\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m model(input_tensor)\n\u001b[1;32m---> 28\u001b[0m activation_map \u001b[38;5;241m=\u001b[39m \u001b[43mcam_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m result \u001b[38;5;241m=\u001b[39m overlay_mask(to_pil_image(image), to_pil_image(activation_map[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m), alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[1;32mD:\\Programs\\anaconda\\lib\\site-packages\\torchcam\\methods\\core.py:143\u001b[0m, in \u001b[0;36m_CAM.__call__\u001b[1;34m(self, class_idx, scores, normalized)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precheck(class_idx, scores)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Compute CAM\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programs\\anaconda\\lib\\site-packages\\torchcam\\methods\\core.py:158\u001b[0m, in \u001b[0;36m_CAM.compute_cams\u001b[1;34m(self, class_idx, scores, normalized)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m\"\"\"Compute the CAM for a specific output class\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    List[torch.Tensor]: list of class activation maps, one for each hooked layer\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Get map weight & unsqueeze it\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m cams: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m weight, activation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_a):\n",
      "File \u001b[1;32mD:\\Programs\\anaconda\\lib\\site-packages\\torchcam\\methods\\gradient.py:292\u001b[0m, in \u001b[0;36mSmoothGradCAMpp._get_weights\u001b[1;34m(self, class_idx, scores)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Alpha coefficient for each pixel\u001b[39;00m\n\u001b[0;32m    291\u001b[0m spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_a[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 292\u001b[0m alpha \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    293\u001b[0m     g2 \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m g2 \u001b[38;5;241m+\u001b[39m (g3 \u001b[38;5;241m*\u001b[39m act)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m spatial_dims])\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m g2, g3, act \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grad_2, grad_3, init_fmap)\n\u001b[0;32m    295\u001b[0m ]\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Apply pixel coefficient in each weight\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    299\u001b[0m     a\u001b[38;5;241m.\u001b[39msqueeze_(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmul_(torch\u001b[38;5;241m.\u001b[39mrelu(grad\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)))\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(alpha, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_g)\n\u001b[0;32m    301\u001b[0m ]\n",
      "File \u001b[1;32mD:\\Programs\\anaconda\\lib\\site-packages\\torchcam\\methods\\gradient.py:293\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Alpha coefficient for each pixel\u001b[39;00m\n\u001b[0;32m    291\u001b[0m spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_a[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    292\u001b[0m alpha \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 293\u001b[0m     g2 \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m g2 \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\u001b[43mg3\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m spatial_dims])\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m g2, g3, act \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grad_2, grad_3, init_fmap)\n\u001b[0;32m    295\u001b[0m ]\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Apply pixel coefficient in each weight\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    299\u001b[0m     a\u001b[38;5;241m.\u001b[39msqueeze_(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmul_(torch\u001b[38;5;241m.\u001b[39mrelu(grad\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)))\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(alpha, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_g)\n\u001b[0;32m    301\u001b[0m ]\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "def GradCam(img_path):\n",
    "    image = tio.ScalarImage(img_path).data.float().to(device)\n",
    "    image = image.permute((0,3,1,2)).squeeze(dim=3)\n",
    "    \n",
    "    grad_cam(model, [model.fc], image, transformer_name='Monai_Blur')\n",
    "    \n",
    "def grad_cam(model, target_layer, image, transformer_name=None):\n",
    "    if transformer_name:\n",
    "        transformer, _  = prep.get_transformer(transformer_name)\n",
    "\n",
    "        input_tensor = transformer(image.squeeze(0))\n",
    "    elif transformer_name is None:\n",
    "        input_tensor = image\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    cam_extractor = SmoothGradCAMpp(model, target_layer=target_layer)\n",
    "\n",
    "    input_tensor = input_tensor.unsqueeze(dim=0)\n",
    "\n",
    "    out = model(input_tensor)\n",
    "\n",
    "    activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "\n",
    "    result = overlay_mask(to_pil_image(image), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.show()\n",
    "    \n",
    "img_path = os.path.join('../../' + CONFIG['SLICED_DATA_DIR'] + 'center/3/' , df_testset.filename[0].replace('data/raw/flattened\\\\', ''))\n",
    "GradCam(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tensor_to_image = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "for data in test_dataloader:\n",
    "    plt.imshow(data[0].permute(2, 3, 0, 1).squeeze(2))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
